{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Consolidated Scripts for Three Models\n",
        "\n"
      ],
      "metadata": {
        "id": "sJELgiDnQIf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. VGG16 Transfer Learning\n",
        "\n"
      ],
      "metadata": {
        "id": "iUyudO5RQNkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "# !pip install numpy==1.26.4 tensorflow==2.15.0 pandas scikit-learn matplotlib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import os\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils import class_weight\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from PIL import Image\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define hyperparameters\n",
        "data_dir = '/content/drive/MyDrive/data'\n",
        "image_height = 224\n",
        "image_width = 224\n",
        "batch_size = 32\n",
        "n_folds = 5\n",
        "\n",
        "# Load dataset\n",
        "image_paths = []\n",
        "labels = []\n",
        "print(f\"\\nScanning directory: {data_dir}\")\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
        "        file_path = os.path.join(data_dir, filename)\n",
        "        try:\n",
        "            with Image.open(file_path) as img:\n",
        "                img.verify()\n",
        "            image_paths.append(file_path)\n",
        "            label = 1 if filename.lower().startswith('y') else 0\n",
        "            labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping corrupt file: {filename} ({str(e)})\")\n",
        "image_paths = np.array(image_paths)\n",
        "labels = np.array(labels)\n",
        "\n",
        "if len(image_paths) == 0:\n",
        "    raise ValueError(f\"No valid images found in {data_dir}.\")\n",
        "print(f\"\\nLoaded {len(image_paths)} images: {sum(labels)} truffle cracks, {len(labels) - sum(labels)} other cracks\")\n",
        "\n",
        "# Split dataset into train+val (240) and test (60)\n",
        "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "# Define data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.3,\n",
        "    height_shift_range=0.3,\n",
        "    shear_range=0.3,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.7, 1.3]\n",
        ")\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# Cross-validation\n",
        "fold_accuracies = []\n",
        "fold_precisions = []\n",
        "fold_recalls = []\n",
        "fold_f1_scores = []\n",
        "fold_aucs = []\n",
        "fold_histories = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_val_paths, train_val_labels)):\n",
        "    print(f\"\\nTraining Fold {fold + 1}/{n_folds}\")\n",
        "    train_paths, val_paths = train_val_paths[train_idx], train_val_paths[val_idx]\n",
        "    train_labels, val_labels = train_val_labels[train_idx], train_val_labels[val_idx]\n",
        "\n",
        "    train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': train_paths, 'class': train_labels.astype(str)}),\n",
        "        x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "        batch_size=batch_size, class_mode='binary', shuffle=True\n",
        "    )\n",
        "    val_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': val_paths, 'class': val_labels.astype(str)}),\n",
        "        x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "        batch_size=batch_size, class_mode='binary', shuffle=False\n",
        "    )\n",
        "\n",
        "    # Build VGG16 model\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3))\n",
        "    base_model.trainable = False\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(256, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(\n",
        "        train_generator, epochs=50, validation_data=val_generator,\n",
        "        callbacks=[early_stopping, lr_schedule], class_weight=class_weight_dict, verbose=1\n",
        "    )\n",
        "    fold_histories.append(history.history)\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    val_predictions = model.predict(val_generator, verbose=0)\n",
        "    val_predictions_binary = (val_predictions > 0.5).astype(int).flatten()\n",
        "    val_true = val_labels\n",
        "    fold_accuracies.append(accuracy_score(val_true, val_predictions_binary))\n",
        "    fold_precisions.append(precision_score(val_true, val_predictions_binary, zero_division=0))\n",
        "    fold_recalls.append(recall_score(val_true, val_predictions_binary, zero_division=0))\n",
        "    fold_f1_scores.append(f1_score(val_true, val_predictions_binary, zero_division=0))\n",
        "    fold_aucs.append(roc_auc_score(val_true, val_predictions))\n",
        "    print(f\"Fold {fold + 1} - Accuracy: {fold_accuracies[-1]:.3f}, Precision: {fold_precisions[-1]:.3f}, \"\n",
        "          f\"Recall: {fold_recalls[-1]:.3f}, F1 Score: {fold_f1_scores[-1]:.3f}, AUC: {fold_aucs[-1]:.3f}\")\n",
        "\n",
        "# Print cross-validation results\n",
        "print(\"\\nCross-Validation Results (Mean ± Std):\")\n",
        "print(f\"Accuracy: {np.mean(fold_accuracies):.3f} ± {np.std(fold_accuracies):.3f}\")\n",
        "print(f\"Precision: {np.mean(fold_precisions):.3f} ± {np.std(fold_precisions):.3f}\")\n",
        "print(f\"Recall: {np.mean(fold_recalls):.3f} ± {np.std(fold_recalls):.3f}\")\n",
        "print(f\"F1 Score: {np.mean(fold_f1_scores):.3f} ± {np.std(fold_f1_scores):.3f}\")\n",
        "print(f\"AUC: {np.mean(fold_aucs):.3f} ± {np.std(fold_aucs):.3f}\")\n",
        "\n",
        "# Train final model on full train+val data\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({'filename': train_val_paths, 'class': train_val_labels.astype(str)}),\n",
        "    x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "    batch_size=batch_size, class_mode='binary', shuffle=True\n",
        ")\n",
        "model = models.Sequential([\n",
        "    VGG16(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.layers[0].trainable = False\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "model.fit(train_generator, epochs=50, callbacks=[early_stopping, lr_schedule], class_weight=class_weight_dict, verbose=1)\n",
        "model.save('/content/drive/MyDrive/truffle_classification_model_vgg16.keras')\n",
        "\n",
        "# Test set evaluation\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({'filename': test_paths, 'class': test_labels.astype(str)}),\n",
        "    x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "    batch_size=batch_size, class_mode='binary', shuffle=False\n",
        ")\n",
        "test_predictions = model.predict(test_generator, verbose=1)\n",
        "test_predictions_binary = (test_predictions > 0.5).astype(int).flatten()\n",
        "test_true = test_generator.classes\n",
        "\n",
        "# Calculate test metrics\n",
        "test_accuracy = accuracy_score(test_true, test_predictions_binary)\n",
        "test_precision = precision_score(test_true, test_predictions_binary, zero_division=0)\n",
        "test_recall = recall_score(test_true, test_predictions_binary, zero_division=0)\n",
        "test_f1 = f1_score(test_true, test_predictions_binary, zero_division=0)\n",
        "test_auc = roc_auc_score(test_true, test_predictions)\n",
        "print(f\"\\nVGG16 Test Results:\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.3f}\")\n",
        "print(f\"Test AUC: {test_auc:.3f}\")\n",
        "\n",
        "# Optimize recall\n",
        "threshold = 0.3\n",
        "test_predictions_binary_opt = (test_predictions > threshold).astype(int).flatten()\n",
        "test_recall_opt = recall_score(test_true, test_predictions_binary_opt, zero_division=0)\n",
        "print(f\"Test Recall (threshold={threshold}): {test_recall_opt:.3f}\")\n",
        "\n",
        "# Generate figures\n",
        "fpr, tpr, _ = roc_curve(test_true, test_predictions)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {test_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('VGG16 ROC Curve (Test Set)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.savefig('/content/drive/MyDrive/roc_curve_vgg16.png')\n",
        "plt.close()\n",
        "\n",
        "cm = confusion_matrix(test_true, test_predictions_binary)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Other Cracks', 'Truffle Cracks'])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title('VGG16 Confusion Matrix (Test Set)')\n",
        "plt.savefig('/content/drive/MyDrive/confusion_matrix_vgg16.png')\n",
        "plt.close()\n",
        "\n",
        "min_epochs = min(len(h['accuracy']) for h in fold_histories)\n",
        "avg_train_acc = np.mean([h['accuracy'][:min_epochs] for h in fold_histories], axis=0)\n",
        "avg_val_acc = np.mean([h['val_accuracy'][:min_epochs] for h in fold_histories], axis=0)\n",
        "avg_train_loss = np.mean([h['loss'][:min_epochs] for h in fold_histories], axis=0)\n",
        "avg_val_loss = np.mean([h['val_loss'][:min_epochs] for h in fold_histories], axis=0)\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(avg_train_acc, label='Training Accuracy')\n",
        "plt.plot(avg_val_acc, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title(f'VGG16 Average Accuracy Across Folds')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(avg_train_loss, label='Training Loss')\n",
        "plt.plot(avg_val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title(f'VGG16 Average Loss Across Folds')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/training_plots_vgg16.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "QjomkYCzQJfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83f276f0-6e9d-49e9-ca55-d71e2b095c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "Scanning directory: /content/drive/MyDrive/data\n",
            "\n",
            "Loaded 300 images: 150 truffle cracks, 150 other cracks\n",
            "Class weights: {0: np.float64(1.0), 1: np.float64(1.0)}\n",
            "\n",
            "Training Fold 1/5\n",
            "Found 192 validated image filenames belonging to 2 classes.\n",
            "Found 48 validated image filenames belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n",
            "Epoch 1/50\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19s/step - accuracy: 0.4622 - auc: 0.4762 - loss: 5.2279 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. SVM with MobileNetV2 Features\n",
        "\n"
      ],
      "metadata": {
        "id": "ThSE0KhzQc_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "# !pip install numpy==1.26.4 tensorflow==2.15.0 pandas scikit-learn matplotlib joblib\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "import os\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.utils import class_weight\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from joblib import dump\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define hyperparameters\n",
        "data_dir = '/content/drive/MyDrive/data'\n",
        "image_height = 224\n",
        "image_width = 224\n",
        "batch_size = 32\n",
        "n_folds = 5\n",
        "\n",
        "# Load dataset\n",
        "image_paths = []\n",
        "labels = []\n",
        "print(f\"\\nScanning directory: {data_dir}\")\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
        "        file_path = os.path.join(data_dir, filename)\n",
        "        try:\n",
        "            with Image.open(file_path) as img:\n",
        "                img.verify()\n",
        "            image_paths.append(file_path)\n",
        "            label = 1 if filename.lower().startswith('y') else 0\n",
        "            labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping corrupt file: {filename} ({str(e)})\")\n",
        "image_paths = np.array(image_paths)\n",
        "labels = np.array(labels)\n",
        "\n",
        "if len(image_paths) == 0:\n",
        "    raise ValueError(f\"No valid images found in {data_dir}.\")\n",
        "print(f\"\\nLoaded {len(image_paths)} images: {sum(labels)} truffle cracks, {len(labels) - sum(labels)} other cracks\")\n",
        "\n",
        "# Split dataset\n",
        "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "# Define data generators\n",
        "datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# Load MobileNetV2 for feature extraction\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3))\n",
        "base_model.trainable = False\n",
        "feature_model = tf.keras.Sequential([base_model, tf.keras.layers.GlobalAveragePooling2D()])\n",
        "\n",
        "def extract_features(generator):\n",
        "    features = feature_model.predict(generator, verbose=1)\n",
        "    labels = generator.classes\n",
        "    return features, labels\n",
        "\n",
        "# Cross-validation\n",
        "fold_accuracies = []\n",
        "fold_precisions = []\n",
        "fold_recalls = []\n",
        "fold_f1_scores = []\n",
        "fold_aucs = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_val_paths, train_val_labels)):\n",
        "    print(f\"\\nTraining Fold {fold + 1}/{n_folds}\")\n",
        "    train_paths, val_paths = train_val_paths[train_idx], train_val_paths[val_idx]\n",
        "    train_labels, val_labels = train_val_labels[train_idx], train_val_labels[val_idx]\n",
        "\n",
        "    train_generator = datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': train_paths, 'class': train_labels.astype(str)}),\n",
        "        x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "        batch_size=batch_size, class_mode='binary', shuffle=False\n",
        "    )\n",
        "    val_generator = datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': val_paths, 'class': val_labels.astype(str)}),\n",
        "        x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "        batch_size=batch_size, class_mode='binary', shuffle=False\n",
        "    )\n",
        "\n",
        "    train_features, train_labels = extract_features(train_generator)\n",
        "    val_features, val_labels = extract_features(val_generator)\n",
        "\n",
        "    svm = SVC(kernel='linear', probability=True, class_weight=class_weight_dict, random_state=42)\n",
        "    svm.fit(train_features, train_labels)\n",
        "\n",
        "    val_preds = svm.predict_proba(val_features)[:, 1]\n",
        "    val_preds_binary = (val_preds > 0.5).astype(int)\n",
        "    fold_accuracies.append(accuracy_score(val_labels, val_preds_binary))\n",
        "    fold_precisions.append(precision_score(val_labels, val_preds_binary, zero_division=0))\n",
        "    fold_recalls.append(recall_score(val_labels, val_preds_binary, zero_division=0))\n",
        "    fold_f1_scores.append(f1_score(val_labels, val_preds_binary, zero_division=0))\n",
        "    fold_aucs.append(roc_auc_score(val_labels, val_preds))\n",
        "    print(f\"Fold {fold + 1} - Accuracy: {fold_accuracies[-1]:.3f}, Precision: {fold_precisions[-1]:.3f}, \"\n",
        "          f\"Recall: {fold_recalls[-1]:.3f}, F1 Score: {fold_f1_scores[-1]:.3f}, AUC: {fold_aucs[-1]:.3f}\")\n",
        "\n",
        "# Print cross-validation results\n",
        "print(\"\\nCross-Validation Results (Mean ± Std):\")\n",
        "print(f\"Accuracy: {np.mean(fold_accuracies):.3f} ± {np.std(fold_accuracies):.3f}\")\n",
        "print(f\"Precision: {np.mean(fold_precisions):.3f} ± {np.std(fold_precisions):.3f}\")\n",
        "print(f\"Recall: {np.mean(fold_recalls):.3f} ± {np.std(fold_recalls):.3f}\")\n",
        "print(f\"F1 Score: {np.mean(fold_f1_scores):.3f} ± {np.std(fold_f1_scores):.3f}\")\n",
        "print(f\"AUC: {np.mean(fold_aucs):.3f} ± {np.std(fold_aucs):.3f}\")\n",
        "\n",
        "# Train final SVM on full train+val data\n",
        "train_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({'filename': train_val_paths, 'class': train_val_labels.astype(str)}),\n",
        "    x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "    batch_size=batch_size, class_mode='binary', shuffle=False\n",
        ")\n",
        "train_features, train_labels = extract_features(train_generator)\n",
        "svm = SVC(kernel='linear', probability=True, class_weight=class_weight_dict, random_state=42)\n",
        "svm.fit(train_features, train_labels)\n",
        "dump(svm, '/content/drive/MyDrive/svm_model.joblib')\n",
        "\n",
        "# Test set evaluation\n",
        "test_generator = datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({'filename': test_paths, 'class': test_labels.astype(str)}),\n",
        "    x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "    batch_size=batch_size, class_mode='binary', shuffle=False\n",
        ")\n",
        "test_features, test_labels = extract_features(test_generator)\n",
        "test_preds = svm.predict_proba(test_features)[:, 1]\n",
        "test_preds_binary = (test_preds > 0.5).astype(int)\n",
        "decision_scores = svm.decision_function(test_features)\n",
        "\n",
        "# Calculate test metrics\n",
        "test_accuracy = accuracy_score(test_labels, test_preds_binary)\n",
        "test_precision = precision_score(test_labels, test_preds_binary, zero_division=0)\n",
        "test_recall = recall_score(test_labels, test_preds_binary, zero_division=0)\n",
        "test_f1 = f1_score(test_labels, test_preds_binary, zero_division=0)\n",
        "test_auc = roc_auc_score(test_labels, test_preds)\n",
        "print(f\"\\nSVM Test Results (default threshold=0.5):\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.3f}\")\n",
        "print(f\"Test AUC: {test_auc:.3f}\")\n",
        "\n",
        "# Optimize recall at threshold=-0.3\n",
        "threshold = -0.3\n",
        "test_preds_binary_opt = (decision_scores > threshold).astype(int)\n",
        "test_accuracy_opt = accuracy_score(test_labels, test_preds_binary_opt)\n",
        "test_precision_opt = precision_score(test_labels, test_preds_binary_opt, zero_division=0)\n",
        "test_recall_opt = recall_score(test_labels, test_preds_binary_opt, zero_division=0)\n",
        "test_f1_opt = f1_score(test_labels, test_preds_binary_opt, zero_division=0)\n",
        "print(f\"\\nSVM Test Results (threshold=-0.3):\")\n",
        "print(f\"Test Accuracy: {test_accuracy_opt:.3f}\")\n",
        "print(f\"Test Precision: {test_precision_opt:.3f}\")\n",
        "print(f\"Test Recall: {test_recall_opt:.3f}\")\n",
        "print(f\"Test F1 Score: {test_f1_opt:.3f}\")\n",
        "print(f\"Test AUC: {test_auc:.3f}\")\n",
        "\n",
        "# Generate figures\n",
        "fpr, tpr, _ = roc_curve(test_labels, test_preds)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {test_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('SVM ROC Curve (Test Set)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.savefig('/content/drive/MyDrive/roc_curve_svm.png')\n",
        "plt.close()\n",
        "\n",
        "cm = confusion_matrix(test_labels, test_preds_binary_opt)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Other Cracks', 'Truffle Cracks'])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title('SVM Confusion Matrix (Test Set, threshold=-0.3)')\n",
        "plt.savefig('/content/drive/MyDrive/confusion_matrix_svm.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "kln6T_WOQd43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. EfficientNetV2B0\n",
        "\n"
      ],
      "metadata": {
        "id": "M2hyciSmQptn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "# !pip install numpy==1.26.4 tensorflow==2.15.0 pandas scikit-learn matplotlib tqdm\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
        "import os\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils import class_weight\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Enable mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define hyperparameters\n",
        "data_dir = '/content/drive/MyDrive/data'\n",
        "image_height = 224\n",
        "image_width = 224\n",
        "batch_size = 16\n",
        "n_folds = 5\n",
        "initial_learning_rate = 3e-4\n",
        "\n",
        "# Load dataset\n",
        "image_paths = []\n",
        "labels = []\n",
        "print(f\"\\nScanning directory: {data_dir}\")\n",
        "for filename in os.listdir(data_dir):\n",
        "    if filename.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
        "        file_path = os.path.join(data_dir, filename)\n",
        "        try:\n",
        "            with Image.open(file_path) as img:\n",
        "                img.verify()\n",
        "            image_paths.append(file_path)\n",
        "            label = 1 if filename.lower().startswith('y') else 0\n",
        "            labels.append(label)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping corrupt file: {filename} ({str(e)})\")\n",
        "image_paths = np.array(image_paths)\n",
        "labels = np.array(labels)\n",
        "\n",
        "if len(image_paths) == 0:\n",
        "    raise ValueError(f\"No valid images found in {data_dir}.\")\n",
        "print(f\"\\nLoaded {len(image_paths)} images: {sum(labels)} truffle cracks, {len(labels) - sum(labels)} other cracks\")\n",
        "\n",
        "# Split dataset\n",
        "train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(\n",
        "    image_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "# Define data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=[0.8, 1.2]\n",
        ")\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "tta_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=5,\n",
        "    width_shift_range=0.05,\n",
        "    height_shift_range=0.05,\n",
        "    zoom_range=0.05,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "# Cross-validation\n",
        "fold_accuracies = []\n",
        "fold_precisions = []\n",
        "fold_recalls = []\n",
        "fold_f1_scores = []\n",
        "fold_aucs = []\n",
        "fold_histories = []\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(train_val_paths, train_val_labels)):\n",
        "    print(f\"\\nTraining Fold {fold + 1}/{n_folds}\")\n",
        "    train_paths, val_paths = train_val_paths[train_idx], train_val_paths[val_idx]\n",
        "    train_labels, val_labels = train_val_labels[train_idx], train_val_labels[val_idx]\n",
        "\n",
        "    train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': train_paths, 'class': train_labels.astype(str)}),\n",
        "        x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "        batch_size=batch_size, class_mode='binary', shuffle=True\n",
        "    )\n",
        "    val_generator = test_datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': val_paths, 'class': val_labels.astype(str)}),\n",
        "        x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "        batch_size=batch_size, class_mode='binary', shuffle=False\n",
        "    )\n",
        "    val_tta_generator = tta_datagen.flow_from_dataframe(\n",
        "        dataframe=pd.DataFrame({'filename': val_paths, 'class': val_labels.astype(str)}),\n",
        "        x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "        batch_size=batch_size, class_mode='binary', shuffle=False\n",
        "    )\n",
        "\n",
        "    # Build EfficientNetV2B0 model\n",
        "    base_model = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3))\n",
        "    base_model.trainable = False\n",
        "    inputs = tf.keras.Input(shape=(image_height, image_width, 3))\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "    x = layers.Dropout(0.7)(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "\n",
        "    lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=8, min_lr=1e-6)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=12, restore_best_weights=True)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator, epochs=50, validation_data=val_generator,\n",
        "        callbacks=[early_stopping, lr_schedule], class_weight=class_weight_dict, verbose=1\n",
        "    )\n",
        "    fold_histories.append(history.history)\n",
        "\n",
        "    # Evaluate with TTA\n",
        "    val_predictions = np.mean([model.predict(val_tta_generator, verbose=0) for _ in range(5)], axis=0)\n",
        "    val_predictions_binary = (val_predictions > 0.5).astype(int).flatten()\n",
        "    val_true = val_labels\n",
        "    fold_accuracies.append(accuracy_score(val_true, val_predictions_binary))\n",
        "    fold_precisions.append(precision_score(val_true, val_predictions_binary, zero_division=0))\n",
        "    fold_recalls.append(recall_score(val_true, val_predictions_binary, zero_division=0))\n",
        "    fold_f1_scores.append(f1_score(val_true, val_predictions_binary, zero_division=0))\n",
        "    fold_aucs.append(roc_auc_score(val_true, val_predictions))\n",
        "    print(f\"Fold {fold + 1} - Accuracy: {fold_accuracies[-1]:.3f}, Precision: {fold_precisions[-1]:.3f}, \"\n",
        "          f\"Recall: {fold_recalls[-1]:.3f}, F1 Score: {fold_f1_scores[-1]:.3f}, AUC: {fold_aucs[-1]:.3f}\")\n",
        "\n",
        "# Print cross-validation results\n",
        "print(\"\\nCross-Validation Results (Mean ± Std):\")\n",
        "print(f\"Accuracy: {np.mean(fold_accuracies):.3f} ± {np.std(fold_accuracies):.3f}\")\n",
        "print(f\"Precision: {np.mean(fold_precisions):.3f} ± {np.std(fold_precisions):.3f}\")\n",
        "print(f\"Recall: {np.mean(fold_recalls):.3f} ± {np.std(fold_recalls):.3f}\")\n",
        "print(f\"F1 Score: {np.mean(fold_f1_scores):.3f} ± {np.std(fold_f1_scores):.3f}\")\n",
        "print(f\"AUC: {np.mean(fold_aucs):.3f} ± {np.std(fold_aucs):.3f}\")\n",
        "\n",
        "# Train final model on full train+val data\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({'filename': train_val_paths, 'class': train_val_labels.astype(str)}),\n",
        "    x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "    batch_size=batch_size, class_mode='binary', shuffle=True\n",
        ")\n",
        "base_model = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(image_height, image_width, 3))\n",
        "base_model.trainable = False\n",
        "inputs = tf.keras.Input(shape=(image_height, image_width, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "x = layers.Dropout(0.7)(x)\n",
        "outputs = layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n",
        "model = Model(inputs, outputs)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=initial_learning_rate),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
        "model.fit(train_generator, epochs=50, callbacks=[early_stopping, lr_schedule], class_weight=class_weight_dict, verbose=1)\n",
        "model.save('/content/drive/MyDrive/truffle_classification_model_effnet.keras')\n",
        "\n",
        "# Test set evaluation with TTA\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({'filename': test_paths, 'class': test_labels.astype(str)}),\n",
        "    x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "    batch_size=batch_size, class_mode='binary', shuffle=False\n",
        ")\n",
        "test_tta_generator = tta_datagen.flow_from_dataframe(\n",
        "    dataframe=pd.DataFrame({'filename': test_paths, 'class': test_labels.astype(str)}),\n",
        "    x_col='filename', y_col='class', target_size=(image_height, image_width),\n",
        "    batch_size=batch_size, class_mode='binary', shuffle=False\n",
        ")\n",
        "test_predictions = np.mean([model.predict(test_tta_generator, verbose=1) for _ in range(5)], axis=0)\n",
        "test_predictions_binary = (test_predictions > 0.5).astype(int).flatten()\n",
        "test_true = test_labels\n",
        "\n",
        "# Calculate test metrics\n",
        "test_accuracy = accuracy_score(test_true, test_predictions_binary)\n",
        "test_precision = precision_score(test_true, test_predictions_binary, zero_division=0)\n",
        "test_recall = recall_score(test_true, test_predictions_binary, zero_division=0)\n",
        "test_f1 = f1_score(test_true, test_predictions_binary, zero_division=0)\n",
        "test_auc = roc_auc_score(test_true, test_predictions)\n",
        "print(f\"\\nEfficientNetV2B0 Test Results (with TTA):\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
        "print(f\"Test Precision: {test_precision:.3f}\")\n",
        "print(f\"Test Recall: {test_recall:.3f}\")\n",
        "print(f\"Test F1 Score: {test_f1:.3f}\")\n",
        "print(f\"Test AUC: {test_auc:.3f}\")\n",
        "\n",
        "# Generate figures\n",
        "fpr, tpr, _ = roc_curve(test_true, test_predictions)\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {test_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('EfficientNetV2B0 ROC Curve (Test Set)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.savefig('/content/drive/MyDrive/roc_curve_effnet.png')\n",
        "plt.close()\n",
        "\n",
        "cm = confusion_matrix(test_true, test_predictions_binary)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Other Cracks', 'Truffle Cracks'])\n",
        "disp.plot(cmap='Blues')\n",
        "plt.title('EfficientNetV2B0 Confusion Matrix (Test Set)')\n",
        "plt.savefig('/content/drive/MyDrive/confusion_matrix_effnet.png')\n",
        "plt.close()\n",
        "\n",
        "min_epochs = min(len(h['accuracy']) for h in fold_histories)\n",
        "avg_train_acc = np.mean([h['accuracy'][:min_epochs] for h in fold_histories], axis=0)\n",
        "avg_val_acc = np.mean([h['val_accuracy'][:min_epochs] for h in fold_histories], axis=0)\n",
        "avg_train_loss = np.mean([h['loss'][:min_epochs] for h in fold_histories], axis=0)\n",
        "avg_val_loss = np.mean([h['val_loss'][:min_epochs] for h in fold_histories], axis=0)\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(avg_train_acc, label='Training Accuracy')\n",
        "plt.plot(avg_val_acc, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.title(f'EfficientNetV2B0 Average Accuracy Across Folds')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(avg_train_loss, label='Training Loss')\n",
        "plt.plot(avg_val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title(f'EfficientNetV2B0 Average Loss Across Folds')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/training_plots_effnet.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "DNX9Fpg0QrNo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
